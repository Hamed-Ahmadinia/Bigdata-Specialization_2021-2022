{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Mining information from Text Data - Hamed Ahmadinia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will explore and analyze the information stored in a particular dataset. In this case the ACL Anthology dataset (https://aclanthology.org/). We will explore different techniques for obtainingn valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 1000 abstracts from the whole dataset. Find the similar items using pairwise Jaccard similarities, MinHash and LSH (vectorized versions) .\n",
    "\n",
    "   1. Compare the performance in time and the results for *k*-shingles = 3, 5 and 10, for the three methods and similarity thresholds *s*=0.9 and 0.95. Use 50 hashing functions. Comment your results. \n",
    "      \n",
    "   2. Compare the results obtained for MinHash and LSH for different similarity thresholds *s* = 0.5, 0.9 and 0.95  and 50, 100 and 200 hashing functions. Comment your results.\n",
    "   \n",
    "   3. For MinHashing using 100 hashing functions and *s* = 0.5 and 0.9,  find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each *s*. Describe the obtained clusters, are they different?. Select randomly at least 5 abstracts per cluster, upon visual inspection, what are the main topics?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bibtexparser in /home/hamed/anaconda3/lib/python3.8/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: future>=0.16.0 in /home/hamed/anaconda3/lib/python3.8/site-packages (from bibtexparser) (0.18.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /home/hamed/anaconda3/lib/python3.8/site-packages (from bibtexparser) (2.4.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bibtexparser # We use this library to read bib file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "\n",
    "# We unziped the giz file and then we used bib file on our local machine\n",
    "with open('anthology+abstracts.bib') as bibtex_file:\n",
    "    bib_database = bibtexparser.bparser.BibTexParser(common_strings=True).parse_file(bibtex_file) # Read bib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>editor</th>\n",
       "      <th>title</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pages</th>\n",
       "      <th>doi</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>author</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>language</th>\n",
       "      <th>number</th>\n",
       "      <th>isbn</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.0</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>woah-2021-online</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.1</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exploiting Auxiliary Data for Offensive Langua...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>singh-li-2021-exploiting</td>\n",
       "      <td>Offensive language detection (OLD) has receive...</td>\n",
       "      <td>1--5</td>\n",
       "      <td>10.18653/v1/2021.woah-1.1</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Singh, Sumer  and\\nLi, Sheng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>hahn-etal-2021-modeling</td>\n",
       "      <td>Hate speech and profanity detection suffer fro...</td>\n",
       "      <td>6--16</td>\n",
       "      <td>10.18653/v1/2021.woah-1.2</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{H}ate{BERT}: Retraining {BERT} for Abusive La...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>caselli-etal-2021-hatebert</td>\n",
       "      <td>We introduce HateBERT, a re-trained BERT model...</td>\n",
       "      <td>17--25</td>\n",
       "      <td>10.18653/v1/2021.woah-1.3</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>kirk-etal-2021-memes</td>\n",
       "      <td>Hateful memes pose a unique challenge for curr...</td>\n",
       "      <td>26--35</td>\n",
       "      <td>10.18653/v1/2021.woah-1.4</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url  \\\n",
       "0  https://aclanthology.org/2021.woah-1.0   \n",
       "1  https://aclanthology.org/2021.woah-1.1   \n",
       "2  https://aclanthology.org/2021.woah-1.2   \n",
       "3  https://aclanthology.org/2021.woah-1.3   \n",
       "4  https://aclanthology.org/2021.woah-1.4   \n",
       "\n",
       "                                   publisher address  year   month  \\\n",
       "0  Association for Computational Linguistics  Online  2021  August   \n",
       "1  Association for Computational Linguistics  Online  2021  August   \n",
       "2  Association for Computational Linguistics  Online  2021  August   \n",
       "3  Association for Computational Linguistics  Online  2021  August   \n",
       "4  Association for Computational Linguistics  Online  2021  August   \n",
       "\n",
       "                                              editor  \\\n",
       "0  Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               title      ENTRYTYPE  \\\n",
       "0  Proceedings of the 5th Workshop on Online Abus...    proceedings   \n",
       "1  Exploiting Auxiliary Data for Offensive Langua...  inproceedings   \n",
       "2  Modeling Profanity and Hate Speech in Social M...  inproceedings   \n",
       "3  {H}ate{BERT}: Retraining {BERT} for Abusive La...  inproceedings   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...  inproceedings   \n",
       "\n",
       "                           ID  \\\n",
       "0            woah-2021-online   \n",
       "1    singh-li-2021-exploiting   \n",
       "2     hahn-etal-2021-modeling   \n",
       "3  caselli-etal-2021-hatebert   \n",
       "4        kirk-etal-2021-memes   \n",
       "\n",
       "                                            abstract   pages  \\\n",
       "0                                                NaN     NaN   \n",
       "1  Offensive language detection (OLD) has receive...    1--5   \n",
       "2  Hate speech and profanity detection suffer fro...   6--16   \n",
       "3  We introduce HateBERT, a re-trained BERT model...  17--25   \n",
       "4  Hateful memes pose a unique challenge for curr...  26--35   \n",
       "\n",
       "                         doi  \\\n",
       "0                        NaN   \n",
       "1  10.18653/v1/2021.woah-1.1   \n",
       "2  10.18653/v1/2021.woah-1.2   \n",
       "3  10.18653/v1/2021.woah-1.3   \n",
       "4  10.18653/v1/2021.woah-1.4   \n",
       "\n",
       "                                           booktitle  \\\n",
       "0                                                NaN   \n",
       "1  Proceedings of the 5th Workshop on Online Abus...   \n",
       "2  Proceedings of the 5th Workshop on Online Abus...   \n",
       "3  Proceedings of the 5th Workshop on Online Abus...   \n",
       "4  Proceedings of the 5th Workshop on Online Abus...   \n",
       "\n",
       "                                              author volume journal language  \\\n",
       "0                                                NaN    NaN     NaN      NaN   \n",
       "1                       Singh, Sumer  and\\nLi, Sheng    NaN     NaN      NaN   \n",
       "2  Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...    NaN     NaN      NaN   \n",
       "3  Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...    NaN     NaN      NaN   \n",
       "4  Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...    NaN     NaN      NaN   \n",
       "\n",
       "  number isbn note  \n",
       "0    NaN  NaN  NaN  \n",
       "1    NaN  NaN  NaN  \n",
       "2    NaN  NaN  NaN  \n",
       "3    NaN  NaN  NaN  \n",
       "4    NaN  NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time\n",
    "\n",
    "data=pd.DataFrame(bib_database.entries) # Create pandas DataFrame from bib file\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 1000 random abstract from acl anthology\n",
    "\n",
    "abstract = [\" \".join(x.lower().split()) for x in data['abstract'].dropna().to_numpy()] # delete exceed space, tab and line\n",
    "abstract = [\"\".join([i for i in x if i.isalpha() or i==' ']) for x in abstract]# delete non alphabet and {'\\/$#%}\n",
    "abstract = [x for x in abstract if len(x)>100]\n",
    "shuffle(abstract) # make random abstracts\n",
    "abstract = abstract[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in largescale educational assessments the use of automated scoring has recently become quite common while the majority of student responses can be processed and scored without difficulty there are a small number of responses that have atypical characteristics that make it difficult for an automated scoring system to assign a correct score we describe a pipeline that detects and processes these kinds of responses at runtime we present the most frequent kinds of what are called nonscorable responses along with effective filtering models based on various nlp and speech processing technologies we give an overview of two operational automated scoring systems one for essay scoring and one for speech scoring and describe the filtering models they use finally we present an evaluation and analysis of filtering models used for spoken responses in an assessment of language proficiency'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text, k):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text) - k+1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return set(shingle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' in', 'nsi', ' we', 'hav', 'oft', 'wev', 'ura', ' di', 'ver', 'ose', 'r t', ' al', 'ive', 'hou', 'iza', 'ns ', 'bot', ' ca', 'e o', 'ngr', 'ubc', 'lic', 'tre', 'erb', 'rk ', ' re', ' fa', 'for', 'chr', ' us', 'r d', 'e i', 'cac', 'pti', 'int', ' ou', 'dit', 'e m', 'and', 'rem', 'ndo', 'omp', ' io', ' is', 'cce', 'che', ' en', 's t', 'the', 'ttp', ' fo', 'at ', 'que', 'x i', 'd n', 'k t', 'seq', 'wor', 'tte', 'y f', 'men', 'ark', 'hro', 'ely', 'gen', 'roc', 'res', 's e', 'uti', 'd u', ' ge', 'a s', 'ere', 'miz', 'r b', 'sts', ' ad', ' ht', ' im', 'ete', 'enc', 'ens', 'e t', 'gor', 'nil', 'to ', 'm f', ' to', 'sfo', 'ved', ' of', 'h a', 'lts', 'd o', 'cie', 'gua', 'lle', ' an', 'uto', ' un', 'e c', 'n n', 'cti', 'esu', 'cel', ' ar', 'pro', 'd i', 'ien', 'pel', 'oug', 't o', 'pee', 'sou', 'ono', ' eg', 'ous', 'm o', 'emo', 'ron', 'sof', 'mew', ' ho', ' ma', 'mer', 'zat', ' at', 'be ', 'ms ', 'nsf', 'hes', 'ilm', ' de', 'nch', 'ngu', 'eve', 'ten', 'o a', 'y l', 'sgi', 'hub', 'ame', 'wit', 'thu', 'gh ', 'orm', 'alg', 'inc', 'ila', 'del', 've ', 'ase', 'l l', 'eas', 'uag', 'ge ', 'one', 'ne ', 'rit', 'ese', 'rce', 'ren', 'e x', 'e s', 'com', ' on', 'n e', 'inv', 'l s', 'y t', 'iti', 'es ', 'syn', 'ati', 'e d', 'nte', 'lab', 'opt', 'ers', ' tr', 'ess', 'ts ', 'atu', 'ipe', 'tfa', 'uni', ' ha', 'q i', 'los', 'sim', 'ect', 'el ', 'ze ', 'uen', ' fr', 'ons', 'aut', 'str', 'pac', ' na', 'ck ', 'hm ', 'put', 'app', 'ran', 'ly ', 'mon', 'div', 'k r', 'cur', 'e w', 'sy ', 'vel', 's o', 'omm', 'ara', 'g t', 'rac', 'use', 'is ', 'ava', 'ss ', 'lve', 'mmi', 'ave', 'pt ', 'in ', ' sp', 'ast', ' au', 'ted', ' mo', 'git', 'ude', 'thm', 'sed', 'o b', 'ttl', 'att', ' ap', 'chm', ' pi', 'sul', 'ing', 'he ', 'ide', 'on ', 'we ', ' x ', 'pli', 'nou', 'epe', ' co', 'ral', ' a ', 'nec', 'lgo', 'se ', 'elo', 'gpt', 'oso', 'n h', 'inf', 'eq ', 'lel', 'lm ', 'et ', 'nel', 'o u', 'nti', 'asy', ' th', ' lo', 'tin', 'acc', 'det', 'nd ', 'cro', 'ult', 'lin', 'nvo', 'l i', 'pip', 'ues', 'gre', 'ork', 'lud', ' ea', 'fas', 'ed ', 'ate', 'n w', 's g', 'din', 'len', 'olv', 'ng ', 'abl', 'htt', 'nt ', 'er ', 'ppl', ' gp', 'gai', ' be', 's i', 'd d', 'ple', 'fra', 're ', 'le ', 'tra', 'mpl', 'els', 'siv', ' wi', 'ccu', 'urc', 'rat', ' ga', 'ain', 'oce', 'ble', 'mic', 'eff', 'eco', ' se', 'ion', ' du', 'pea', 'tle', 'mpu', 'te ', 'rse', ' bo', 'cy ', 'iqu', 'rge', 'odi', 'g p', 'd g', 'ore', 'h t', 'vol', 't h', 'ben', 'ine', 'acy', 'eed', 'ode', 'l e', 'ams', 'ent', 'q f', 'how', 'niq', 's h', 'tho', 'owe', ' te', 'g r', 'ue ', 'lan', ' pr', 'clu', 'tor', 'act', 'n t', 'ail', 'ans', 'ene', 'al ', ' av', 'ach', 'icr', 'ler', 't a', 'imi', 'nal', 'an ', 'lop', 'eli', 'us ', 'due', 'ech', ' la', 'e g', 'par', ' t ', 'ces', ' ef', 'chn', 'hni', 'mad', 'p f', 'io ', 'tim', 'a b', 'hma', 'ori', ' pa', 'rme', 'our', ' so', 'set', 'nat', 'cab', 'vai', 'imp', 'add', 'ftf', 'k d', 'bas', 'ssi', 'eat', 'o t', 'mpa', 'ner', 'ugh', ' ng', 'equ', 'end', 'dec', 'ici', 'tio', 'y u', 'ros', 'n a', 'pos', 'eno', 'siz', 'lly', 'ur ', 'dem', 'ona', 'ffi', ' si', 'ica', 'th ', 'nst', 'cod', 'ram', 'gra', 'dou', 's w', 'era', 'of ', 'n p', 'ut ', 'han', 'o l', 'are', 'or ', 'out', 'tec', 'mod', 'tse', 'ang', 'ewo', 'eme', 'e p', 'fer', 'n c', 's d', 'nge', ' ac', 'fic', 'all', 'ade', ' ch', 'd a', 'wid', 'age', 'nce', 'reg', 'ls ', 'f w', 'h p', 'tur', 't g', 'd m', 'arg', 'ddi', 'rop', 'tps', 'ele', 'spe', 'rba', 'opo', 'eg ', 'bco', 'nfe', 'rep', 'lar', 'ync', 'ize', 'cha', ' op', 'e a', 'egr', 'g i', 'ncl', 's a', 'op ', 'cts', 'oss', 'dev', 'ott', ' as', 'mar', 'de ', 'ce ', 'psg', 'ith', 'eck'}\n"
     ]
    }
   ],
   "source": [
    "print(shingle(abstract[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(abstract, k):\n",
    "    return [shingle(x, k) for x in abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles = create_shingles(abstract, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def unionize(shingles): # we union all of shingles and make a set from that, it help to create a pool of shingles\n",
    "    return list(set(itertools.chain(*shingles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = unionize(shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(shingles, pool):\n",
    "    ab_1hot = []\n",
    "    for ab in shingles:\n",
    "        ab_1hot.append([1 if x in ab else 0 for x in pool])\n",
    "    return ab_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each abstract we create an on-hot ecoder with length of pool\n",
    "# this is very sparse vector for each abstract\n",
    "\n",
    "ab_1hot = create_one_hot(shingles, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(ab_1hot[41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_func(size: int):\n",
    "    # function for creating the hash vector/function\n",
    "    hash_ex = list(range(1, len(pool)+1))\n",
    "    shuffle(hash_ex)\n",
    "    return hash_ex\n",
    "\n",
    "def build_minhash_func(pool_size: int, nbits: int):\n",
    "    # function for building multiple minhash vectors\n",
    "    hashes = []\n",
    "    for _ in range(nbits):\n",
    "        hashes.append(create_hash_func(pool_size))\n",
    "    return hashes\n",
    "\n",
    "# we create 50 minhash vectors\n",
    "minhash_func = build_minhash_func(len(pool), 50)\n",
    "\n",
    "def create_hash(vector: list):\n",
    "    # use this function for creating our signatures\n",
    "    signature = []\n",
    "    for func in minhash_func:\n",
    "        for i in range(1, len(pool)+1):\n",
    "            idx = func.index(i)\n",
    "            signature_val = vector[idx]\n",
    "            if signature_val == 1:\n",
    "                signature.append(idx)\n",
    "                break\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signatures(ab_1hot):\n",
    "    ab_sign = []\n",
    "    for ab in ab_1hot:\n",
    "        ab_sign.append(create_hash(ab))\n",
    "    return ab_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.6 s, sys: 220 ms, total: 45.9 s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create signature from each abstract with intuitive and very slow method\n",
    "# as pool size increase, time increase exponentialy\n",
    "\n",
    "ab_sign = create_signatures(ab_1hot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23876404494382023, 0.22666666666666666)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is show that our signature work good for us and it is near real value of jaccard similarity\n",
    "jaccard_similarity(shingles[10], shingles[100]), jaccard_similarity(ab_sign[10], ab_sign[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vector(signature, b):\n",
    "    r = int(len(signature) / b)\n",
    "    # code splitting signature in b parts\n",
    "    subvecs = []\n",
    "    for i in range(0, len(signature), r):\n",
    "        subvecs.append(signature[i : i+r])\n",
    "    return subvecs\n",
    "\n",
    "def split_all(ab_sign, b):\n",
    "    return [split_vector(sign, b) for sign in ab_sign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6414, 2567, 3479, 4650, 3814],\n",
       " [306, 6853, 1990, 1359, 3620],\n",
       " [2324, 2579, 6097, 306, 669],\n",
       " [3241, 5980, 1531, 6046, 6414],\n",
       " [6537, 1419, 110, 5596, 4079],\n",
       " [343, 2539, 2716, 764, 2857],\n",
       " [340, 5039, 2522, 4667, 4539],\n",
       " [2373, 4268, 2189, 356, 6233],\n",
       " [1138, 1372, 5737, 5429, 6610],\n",
       " [2826, 6414, 5245, 878, 4699]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_split = split_all(ab_sign, 10) # we split signiture vector to 10 band and later use in LSH\n",
    "ab_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jac_score(shingles):\n",
    "    jacc_score = []\n",
    "    for i in range(len(shingles)):\n",
    "        for j in range(i, len(shingles)):\n",
    "            jacc_score.append(list([i, j, jaccard_similarity(shingles[i], shingles[j])]))\n",
    "    return pd.DataFrame(jacc_score, columns=[\"abstract_one\", \"abstract_two\", \"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minihash_score(ab_sign):\n",
    "    minihash_score = []\n",
    "    for i in range(len(ab_sign)):\n",
    "        for j in range(i, len(ab_sign)):\n",
    "            minihash_score.append(list([i, j, jaccard_similarity(ab_sign[i], ab_sign[j])]))\n",
    "    return pd.DataFrame(minihash_score, columns=[\"abstract_one\", \"abstract_two\", \"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lsh_score(ab_splits):\n",
    "    lsh_score = []\n",
    "    for i in range(len(ab_splits)):\n",
    "        for j in range(i, len(ab_splits)):\n",
    "            for i_rows, j_rows in zip(ab_splits[i], ab_splits[j]):\n",
    "                if i_rows == j_rows:\n",
    "                    lsh_score.append(list([i,j,jaccard_similarity(ab_sign[i], ab_sign[j])]))\n",
    "                    # we only need one band to match\n",
    "                    break\n",
    "    return pd.DataFrame(lsh_score, columns=[\"abstract_one\", \"abstract_two\", \"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.4 s, sys: 131 ms, total: 37.6 s\n",
      "Wall time: 38 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_one</th>\n",
       "      <th>abstract_two</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [abstract_one, abstract_two, similarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "jac_score = create_jac_score(shingles)\n",
    "jac_score.loc[(jac_score.similarity>.5)&(jac_score.similarity<1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.24 s, sys: 16.1 ms, total: 5.26 s\n",
      "Wall time: 5.39 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_one</th>\n",
       "      <th>abstract_two</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [abstract_one, abstract_two, similarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "minihash_score = create_minihash_score(ab_sign)\n",
    "minihash_score.loc[(minihash_score.similarity>.5)&(minihash_score.similarity<1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 402 ms, sys: 7.84 ms, total: 410 ms\n",
      "Wall time: 445 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_one</th>\n",
       "      <th>abstract_two</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [abstract_one, abstract_two, similarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsh_score = create_lsh_score(ab_split)\n",
    "lsh_score.loc[(lsh_score.similarity>.5)&(lsh_score.similarity<1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create basics to use different methods \n",
    "now time to measure performance and results.\n",
    "\n",
    "\n",
    "We implement minhash with simple method that get struggle in large number of article and shingles.\n",
    "\n",
    "\n",
    "first we need to implement minhash algorithm faster and more efficient ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from binascii import crc32\n",
    "\n",
    "def minh(s, N, prime=4294967311):\n",
    "    max_val = (2**32)-1\n",
    "    perms = [(randint(0,max_val), randint(0,max_val)) for i in range(N)]\n",
    "    vec = []\n",
    "\n",
    "    for n in range(N):\n",
    "        a, b = perms[n][0], perms[n][1]\n",
    "        \n",
    "        vec_ = [float('inf') for i in range(len(s))]\n",
    "        for abstract_idx in range(len(s)):\n",
    "            for i in s[abstract_idx]:\n",
    "                ha = crc32(i.encode('utf-8')) \n",
    "                output = (a * ha + b) % prime\n",
    "                if vec_[abstract_idx] > output:\n",
    "                    vec_[abstract_idx] = output\n",
    "        vec.append(vec_)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2- Compare the results obtained for MinHash and LSH for different similarity thresholds s = 0.5, 0.9 and 0.95 and 50, 100 and 200 hashing functions. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we implement minhash and lsh and trying 50, 100 and 200 hash functions. also use 3,5 and 10 shingles and from result we can say that:\n",
    "\n",
    "- jaccard is precious but slow\n",
    "    as shingle size is increasing jaccard become slower\n",
    "    order of complexity of jaccard is O(n^2)\n",
    "\n",
    "- minihash is much fatter than jaccard and precious is not as good as jaccard although very reasonable.\n",
    "\n",
    "- lsh solve the problem of precious of minhash and also faster than minhash.\n",
    "\n",
    "- as shingles increase runtime become increase much faster and we need to use lsh instead of jaccard and for value more than 10 million almost impossible to use jaccard\n",
    "\n",
    "- as hash functions increase result of lsh becomes more precious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JACCARD(k_shingle=3, jac_threshold=.5):\n",
    "    t0 = time.time()\n",
    "    print(f'start jaccard with {k_shingle}_shingle ===================')\n",
    "    shingles = create_shingles(abstract, k_shingle)\n",
    "        \n",
    "    for i in range(len(shingles)):\n",
    "        for j in range(i,len(shingles)):\n",
    "            jac = jaccard_similarity(shingles[i], shingles[j])\n",
    "            if (jac >jac_threshold) & (jac<1):\n",
    "                print(i, j, jac)\n",
    "    print(\"finish jaccard in: \", time.time() - t0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINHASH(N=50, k_shingle=3, jac_threshold=.5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f'start minHash with {N} hash function and {k_shingle}_shingle ===================')\n",
    "    shingles = create_shingles(abstract, k_shingle)\n",
    "    minhash_func = pd.DataFrame(minh(shingles, N))\n",
    "    print(\"create sinatures: \", time.time() - t0)\n",
    "    \n",
    "    l=len(shingles)\n",
    "    a=[]\n",
    "    for i in range(l):\n",
    "        a.append(minhash_func.iloc[:,i])\n",
    "    for i in range(l):\n",
    "        for j in range(i+1,l):\n",
    "            jac = jaccard_similarity(a[i], a[j])\n",
    "            if (jac >.5) & (jac<1):\n",
    "                print(i, j, jac)\n",
    "    print(\"finish minhash in: \", time.time() - t0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH(N=50, k_shingle=3, jac_threshold=.5, splits=4):\n",
    "    t0 = time.time()\n",
    "    print(f'start LSH with {N} hash function and {k_shingle}_shingle ===================')\n",
    "    shingles = create_shingles(abstract, k_shingle)\n",
    "    m = pd.DataFrame(minh(shingles, N))\n",
    "    print(\"create sinatures: \", time.time() - t0)\n",
    "    t0 = time.time()\n",
    "    b=[]\n",
    "    br=False\n",
    "    l=len(shingles)\n",
    "    for i in range(l):\n",
    "        b.append(split_vector(m.iloc[:,i],splits))\n",
    "\n",
    "    print(\"split complete: \", time.time() - t0)\n",
    "    \n",
    "    for i in range(l):\n",
    "        for j in range(i+1,l):\n",
    "            for idx in range(splits):\n",
    "                if br==True:\n",
    "                    br = False\n",
    "                    break\n",
    "                for i_rows, j_rows in zip(b[i][idx], b[j][idx]):\n",
    "                    if i_rows == j_rows:\n",
    "                        br = True\n",
    "                        jac = jaccard_similarity(shingles[i], shingles[j])\n",
    "                        if (jac >jac_threshold) & (jac<1):\n",
    "                            print(i, j, jac)\n",
    "                        break\n",
    "    print(\"finish lsh in: \", time.time() - t0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start jaccard with 3_shingle ===================\n",
      "finish jaccard in:  40.837918519973755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JACCARD(k_shingle=3, jac_threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start minHash with 50 hash function and 3_shingle ===================\n",
      "create sinatures:  9.123526096343994\n",
      "finish minhash in:  17.518795251846313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MINHASH(N=50, k_shingle=3, jac_threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start LSH with 50 hash function and 3_shingle ===================\n",
      "create sinatures:  8.668784856796265\n",
      "split complete:  0.6250503063201904\n",
      "106 962 0.42191780821917807\n",
      "123 503 0.41233373639661425\n",
      "finish lsh in:  47.83500623703003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LSH(N=50, k_shingle=3, jac_threshold=.4, splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1- Compare the performance in time and the results for k-shingles = 3, 5 and 10, for the three methods and similarity thresholds s=0.9 and 0.95. Use 50 hashing functions. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start jaccard with 3_shingle ===================\n",
      "finish jaccard in:  70.77935671806335\n",
      "\n",
      "start jaccard with 5_shingle ===================\n",
      "finish jaccard in:  72.55189490318298\n",
      "\n",
      "start jaccard with 10_shingle ===================\n",
      "finish jaccard in:  64.1764976978302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JACCARD(k_shingle=3, jac_threshold=.5)\n",
    "JACCARD(k_shingle=5, jac_threshold=.5)\n",
    "JACCARD(k_shingle=10, jac_threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start minHash with 50 hash function and 3_shingle ===================\n",
      "create sinatures:  10.616516828536987\n",
      "finish minhash in:  20.56183123588562\n",
      "\n",
      "start minHash with 50 hash function and 5_shingle ===================\n",
      "create sinatures:  15.003560066223145\n",
      "finish minhash in:  24.707454919815063\n",
      "\n",
      "start minHash with 50 hash function and 10_shingle ===================\n",
      "create sinatures:  19.519854068756104\n",
      "finish minhash in:  29.48997950553894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MINHASH(N=50, k_shingle=3, jac_threshold=.5)\n",
    "MINHASH(N=50, k_shingle=5, jac_threshold=.5)\n",
    "MINHASH(N=50, k_shingle=10, jac_threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start LSH with 50 hash function and 3_shingle ===================\n",
      "create sinatures:  9.783883094787598\n",
      "split complete:  0.412304162979126\n",
      "finish lsh in:  49.506980657577515\n",
      "\n",
      "start LSH with 50 hash function and 5_shingle ===================\n",
      "create sinatures:  16.02803111076355\n",
      "split complete:  0.37647557258605957\n",
      "finish lsh in:  59.94094800949097\n",
      "\n",
      "start LSH with 50 hash function and 10_shingle ===================\n",
      "create sinatures:  18.245524406433105\n",
      "split complete:  0.44815897941589355\n",
      "finish lsh in:  28.59602642059326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LSH(N=50, k_shingle=3, jac_threshold=.5, splits=4)\n",
    "LSH(N=50, k_shingle=5, jac_threshold=.5, splits=4)\n",
    "LSH(N=50, k_shingle=10, jac_threshold=.5, splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start minHash with 100 hash function and 3_shingle ===================\n",
      "create sinatures:  20.665815830230713\n",
      "finish minhash in:  40.56891751289368\n",
      "\n",
      "start minHash with 100 hash function and 5_shingle ===================\n",
      "create sinatures:  30.59933853149414\n",
      "finish minhash in:  48.16977787017822\n",
      "\n",
      "start minHash with 100 hash function and 10_shingle ===================\n",
      "create sinatures:  33.732871294021606\n",
      "finish minhash in:  52.467140674591064\n",
      "\n",
      "start minHash with 200 hash function and 3_shingle ===================\n",
      "create sinatures:  44.410902976989746\n",
      "finish minhash in:  80.19050359725952\n",
      "\n",
      "start minHash with 200 hash function and 5_shingle ===================\n",
      "create sinatures:  63.09355640411377\n",
      "finish minhash in:  99.13037848472595\n",
      "\n",
      "start minHash with 200 hash function and 10_shingle ===================\n",
      "create sinatures:  74.03358745574951\n",
      "finish minhash in:  109.07237935066223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MINHASH(N=100, k_shingle=3, jac_threshold=.5)\n",
    "MINHASH(N=100, k_shingle=5, jac_threshold=.5)\n",
    "MINHASH(N=100, k_shingle=10, jac_threshold=.5)\n",
    "MINHASH(N=200, k_shingle=3, jac_threshold=.5)\n",
    "MINHASH(N=200, k_shingle=5, jac_threshold=.5)\n",
    "MINHASH(N=200, k_shingle=10, jac_threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start LSH with 100 hash function and 3_shingle ===================\n",
      "create sinatures:  20.648046255111694\n",
      "split complete:  0.47182416915893555\n",
      "finish lsh in:  47.13585376739502\n",
      "\n",
      "start LSH with 100 hash function and 5_shingle ===================\n",
      "create sinatures:  31.871016263961792\n",
      "split complete:  0.4316837787628174\n",
      "finish lsh in:  58.93512797355652\n",
      "\n",
      "start LSH with 100 hash function and 10_shingle ===================\n",
      "create sinatures:  38.67946195602417\n",
      "split complete:  0.5153203010559082\n",
      "finish lsh in:  45.597782611846924\n",
      "\n",
      "start LSH with 200 hash function and 3_shingle ===================\n",
      "create sinatures:  42.12301683425903\n",
      "split complete:  0.22830581665039062\n",
      "finish lsh in:  48.016915798187256\n",
      "\n",
      "start LSH with 200 hash function and 5_shingle ===================\n",
      "create sinatures:  62.0220890045166\n",
      "split complete:  0.4754030704498291\n",
      "finish lsh in:  65.30190920829773\n",
      "\n",
      "start LSH with 200 hash function and 10_shingle ===================\n",
      "create sinatures:  74.98971700668335\n",
      "split complete:  0.6402044296264648\n",
      "finish lsh in:  50.79961156845093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LSH(N=100, k_shingle=3, jac_threshold=.5, splits=4)\n",
    "LSH(N=100, k_shingle=5, jac_threshold=.5, splits=4)\n",
    "LSH(N=100, k_shingle=10, jac_threshold=.5, splits=4)\n",
    "LSH(N=200, k_shingle=3, jac_threshold=.5, splits=4)\n",
    "LSH(N=200, k_shingle=5, jac_threshold=.5, splits=4)\n",
    "LSH(N=200, k_shingle=10, jac_threshold=.5, splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3- For MinHashing using 100 hashing functions and s = 0.5 and 0.9, find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create sinatures:  18.717809438705444\n",
      "finish minhash in:  38.61874604225159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "shingles = create_shingles(abstract, 3)\n",
    "minhash_func = pd.DataFrame(minh(shingles, 100))\n",
    "print(\"create sinatures: \", time.time() - t0)\n",
    "\n",
    "l=len(shingles)\n",
    "a=[]\n",
    "jac=[]\n",
    "for i in range(l):\n",
    "    a.append(minhash_func.iloc[:,i])\n",
    "for i in range(l):\n",
    "    for j in range(i+1,l):\n",
    "        jac.append([i, j, jaccard_similarity(a[i], a[j])])\n",
    "\n",
    "print(\"finish minhash in: \", time.time() - t0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_one</th>\n",
       "      <th>abstract_two</th>\n",
       "      <th>jaccard-distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   abstract_one  abstract_two  jaccard-distance\n",
       "0             0             1          0.843931\n",
       "1             0             2          0.857143\n",
       "2             0             3          0.863636\n",
       "3             0             4          0.876404\n",
       "4             0             5          0.936170"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(jac, columns=['abstract_one', 'abstract_two', 'jaccard-distance'])\n",
    "result['jaccard-distance'] = 1 - result['jaccard-distance']\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 2, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "knn = KMeans(n_clusters=3).fit(result[['jaccard-distance']])\n",
    "knn.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 3, 0], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KMeans(n_clusters=5).fit(result[['jaccard-distance']])\n",
    "knn.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_one</th>\n",
       "      <th>abstract_two</th>\n",
       "      <th>jaccard-distance</th>\n",
       "      <th>knn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248411</th>\n",
       "      <td>290</td>\n",
       "      <td>897</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128804</th>\n",
       "      <td>138</td>\n",
       "      <td>534</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125269</th>\n",
       "      <td>134</td>\n",
       "      <td>449</td>\n",
       "      <td>0.675497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45841</th>\n",
       "      <td>46</td>\n",
       "      <td>969</td>\n",
       "      <td>0.675497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45824</th>\n",
       "      <td>46</td>\n",
       "      <td>952</td>\n",
       "      <td>0.675497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422788</th>\n",
       "      <td>607</td>\n",
       "      <td>924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347899</th>\n",
       "      <td>448</td>\n",
       "      <td>924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323123</th>\n",
       "      <td>405</td>\n",
       "      <td>744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214789</th>\n",
       "      <td>244</td>\n",
       "      <td>924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337648</th>\n",
       "      <td>430</td>\n",
       "      <td>744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499500 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        abstract_one  abstract_two  jaccard-distance  knn\n",
       "248411           290           897          0.657718    3\n",
       "128804           138           534          0.666667    3\n",
       "125269           134           449          0.675497    3\n",
       "45841             46           969          0.675497    3\n",
       "45824             46           952          0.675497    3\n",
       "...              ...           ...               ...  ...\n",
       "422788           607           924          1.000000    4\n",
       "347899           448           924          1.000000    4\n",
       "323123           405           744          1.000000    4\n",
       "214789           244           924          1.000000    4\n",
       "337648           430           744          1.000000    4\n",
       "\n",
       "[499500 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['knn'] = knn.labels_\n",
    "result.sort_values(['jaccard-distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lobjectif est letude des causes des disperiodicites des voix du type  qui sont pseudoperiodiques et monophoniques un modele qui explique quantitativement les perturbations des durees de cycles glottiques fait appel aux fluctuations de la tension du muscle vocal or ces fluctuations nexpliquent pas lenrouement qui peut faire suite a une charge vocale ou une laryngite legere par exemple cest pourquoi nous discutons plusieurs modeles qui montrent quune redistribution des amplitudes vibratoires entre le corps et la couverture du pli module les perturbations qui trouvent leur origine au niveau du muscle vocal des simulations a laide dun modele corpscouverture suggerent ainsi que les perturbations des durees des cycles glottiques augmentent avec une redistribution des amplitudes vibratoires de la couverture vers le muscle suite a une redistribution des masses vibrantes du muscle vers la couverture\n"
     ]
    }
   ],
   "source": [
    "print(abstract[436])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expressions with an aspectual variant of a light verb eg take on debt vs have debt are frequent in texts but often difficult to classify between verbal idioms light verb constructions or compositional phrases we investigate the properties of such expressions with a disputed membership and propose a selection of features that determine more satisfactory boundaries between the three categories in this zone assigning the expressions to one of them\n"
     ]
    }
   ],
   "source": [
    "print(abstract[624])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we introduce a novel method for multilingual transfer that utilizes deep contextual embeddings pretrained in an unsupervised fashion while contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts aligning them poses a challenge due to their dynamic nature to this end we construct contextindependent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the contextdependent spaces this mapping readily supports processing of a target language improving transfer by contextaware embeddings our experimental results demonstrate the effectiveness of this approach for zeroshot and fewshot learning of dependency parsing specifically our method consistently outperforms the previous stateoftheart on  tested languages yielding an improvement of  las points on average\n"
     ]
    }
   ],
   "source": [
    "print(abstract[814])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowadays spoken dialogue agents such as communication robots and smart speakers listen to narratives of humans in order for such an agent to be recognized as a listener of narratives and convey the attitude of attentive listening it is necessary to generate responsive utterances moreover responsive utterances can express empathy to narratives and showing an appropriate degree of empathy to narratives is significant for enhancing speakers motivation the degree of empathy shown by responsive utterances is thought to depend on their type however the relation between responsive utterances and degrees of the empathy has not been explored yet this paper describes the classification of responsive utterances based on the degree of empathy in order to explain that relation in this research responsive utterances are classified into five levels based on the effect of utterances and literature on attentive listening quantitative evaluations using  responsive utterances showed the appropriateness of the proposed classification\n"
     ]
    }
   ],
   "source": [
    "print(abstract[410])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyponymy is the cornerstone of taxonomies and concept hierarchies however the extraction of hypernymhyponym pairs from a corpus can be timeconsuming and reconstructing the hierarchical network of a domain is often an extremely complex process this paper presents the development and evaluation of the french ecolexicon semantic sketch grammar essgfr a french hyponymic sketch grammar for sketch engine based on knowledge patterns it offers a userfriendly way of extracting hyponymic pairs in the form of word sketches in any userowned corpus the essgfr contains three times more hyponymic patterns than its english counterpart and has been tested in a multidisciplinary corpus it is thus expected to be domainindependent moreover the following methodological innovations have been included in its development  use of english hyponymic patterns in a parallel corpus to find new french patterns  automatic inclusion of the results of the sketch engine thesaurus to find new variants of the patterns as for its evaluation the essgfr returns  valid hyperonyms and hyponyms measured on  extracted pairs of terms in three different domains\n"
     ]
    }
   ],
   "source": [
    "print(abstract[769])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
